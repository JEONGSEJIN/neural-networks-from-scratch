{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333337 0.33333337 0.33333328]\n",
      " [0.33333337 0.33333334 0.33333325]\n",
      " [0.3333334  0.3333333  0.33333322]\n",
      " [0.33333346 0.33333334 0.3333332 ]]\n",
      "loss: 1.0986122\n",
      "acc: 0.37333333333333335\n",
      "[[-2.1235142e-06 -2.6520061e-06 -5.2077548e-06 -2.6749271e-06\n",
      "  -4.8067532e-06 -2.0273822e-06  4.2844772e-06  1.7412808e-06\n",
      "  -5.9487256e-06 -9.7394388e-07 -2.3774219e-06  1.0583032e-06\n",
      "  -5.5542182e-06 -2.5369964e-06  1.7481290e-06 -1.3927128e-06\n",
      "  -3.7761090e-06 -6.2298295e-06 -2.1019648e-06  2.8360802e-07\n",
      "   5.5108922e-07  2.0665202e-06 -4.4412630e-07 -4.3265886e-06\n",
      "  -1.8380820e-06  3.8453195e-06 -7.0508668e-07 -4.8804714e-07]\n",
      " [ 6.7413907e-06 -1.6443076e-06 -1.3464736e-06 -5.2749765e-06\n",
      "   1.9395427e-06  4.2022793e-06  7.0113424e-06 -1.9438587e-06\n",
      "  -6.3481630e-06  1.7467015e-07 -1.9068970e-06 -4.4897884e-06\n",
      "  -2.0466812e-06  1.7888343e-06  1.2809937e-05 -4.7382559e-06\n",
      "  -1.0589288e-05 -1.0449988e-05 -2.8472425e-06 -7.3624942e-06\n",
      "  -1.2371348e-05 -3.7096042e-06 -2.3458122e-06 -7.0195219e-06\n",
      "  -3.0854414e-06  5.2347815e-07 -7.1944373e-06  2.8220009e-06]]\n",
      "[[ 2.44126738e-07 -1.30739527e-05 -1.29745013e-05  6.65380503e-06\n",
      "  -2.24214750e-06 -2.29658667e-06 -9.05941124e-06 -6.56490101e-06\n",
      "   1.76386420e-05  7.82939651e-07  1.25124125e-05 -9.69642042e-06\n",
      "   7.23733228e-06  5.68687165e-06 -1.78785995e-05  9.48102024e-06\n",
      "   1.95004613e-05  1.77747988e-05  8.30970475e-06 -2.19220492e-05\n",
      "  -1.78762202e-05 -6.17469959e-06  1.76619790e-06  1.56779643e-05\n",
      "  -1.82378460e-06 -9.56131589e-06  1.58995372e-05 -1.07962733e-05]]\n",
      "[[ 0.00000000e+00  0.00000000e+00 -2.23593133e-07  2.37170184e-06\n",
      "   0.00000000e+00 -1.02519198e-06  1.71703914e-06  1.06788684e-06\n",
      "  -1.87591343e-06  2.12648828e-07 -7.06309855e-09  2.52480140e-06\n",
      "   5.97696669e-07  2.67312066e-06  2.54807173e-06 -8.14573085e-08\n",
      "   2.41336215e-06  4.10034716e-07 -8.25767117e-08 -5.24578854e-06\n",
      "  -4.17277579e-06  0.00000000e+00 -2.37830250e-06  0.00000000e+00\n",
      "   2.32884577e-06 -1.03865045e-06 -9.92333298e-07 -2.71343538e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00  2.23015013e-06 -1.78494463e-06\n",
      "  -1.11343148e-07 -3.43931589e-07  5.55903057e-07  3.21756687e-07\n",
      "  -4.42163099e-07 -5.49048862e-10 -2.32178920e-07  4.94046759e-08\n",
      "  -1.02255171e-06 -2.11636848e-06 -1.88545278e-06  9.90835360e-07\n",
      "   6.53559027e-07 -5.71144199e-08 -1.67266080e-06  2.34974618e-06\n",
      "   0.00000000e+00  4.51939542e-07  2.34817026e-06  0.00000000e+00\n",
      "  -1.62983201e-06 -9.04839013e-08  1.05621689e-06 -1.87860769e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00  2.85603079e-07 -7.25942300e-07\n",
      "   0.00000000e+00  3.21281476e-07  1.80365828e-06  1.66330040e-07\n",
      "   5.09651045e-07  1.59943543e-08  4.02282467e-07  0.00000000e+00\n",
      "  -2.79022629e-07 -1.31184527e-06 -6.27796112e-07 -2.27971469e-07\n",
      "   6.64256220e-07 -4.30908500e-07 -5.75980891e-07  3.92805674e-07\n",
      "   0.00000000e+00  0.00000000e+00  1.87034527e-06  0.00000000e+00\n",
      "  -2.87021180e-07 -4.39104127e-07 -5.71725920e-08 -2.11488236e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00 -4.47291853e-07  1.97578197e-06\n",
      "  -5.46025625e-09 -1.98729390e-06 -3.80057628e-07  4.23376775e-07\n",
      "  -2.10443477e-06  5.56163116e-08  0.00000000e+00  3.25296560e-06\n",
      "   2.10186599e-07  3.27356338e-06  1.80014513e-06 -1.48035923e-07\n",
      "   9.08620962e-07  5.44830803e-09  0.00000000e+00 -5.31950445e-06\n",
      "  -3.86473721e-06 -1.13103049e-07 -3.78387108e-06  0.00000000e+00\n",
      "   2.42810802e-06 -3.56380411e-07 -1.53531573e-07 -9.14693317e-07]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.13113515e-06 -1.17790125e-06\n",
      "  -3.07846264e-08 -4.58122340e-09  6.29158251e-07  9.66752935e-08\n",
      "   1.33789229e-08 -2.78067880e-08  9.51834878e-08  0.00000000e+00\n",
      "  -7.07587560e-07 -1.45690558e-06 -1.22564597e-06  3.32902715e-07\n",
      "   2.93696075e-07 -3.83458143e-07 -9.77572313e-07  1.29942430e-06\n",
      "   0.00000000e+00  1.84957358e-07  1.63628295e-06  0.00000000e+00\n",
      "  -8.55901590e-07 -9.53036974e-08  6.50671552e-07 -1.46261175e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00 -4.37829243e-07  1.63983111e-06\n",
      "  -1.62553491e-08 -2.01971807e-06 -6.17344426e-07  2.58082480e-07\n",
      "  -1.88666866e-06  2.13501679e-08  0.00000000e+00  3.10891392e-06\n",
      "   1.06323952e-07  2.98758277e-06  1.41065368e-06 -1.36282651e-07\n",
      "   5.52161794e-07  0.00000000e+00  0.00000000e+00 -4.81526604e-06\n",
      "  -3.39708981e-06 -1.30768470e-07 -3.59840351e-06  0.00000000e+00\n",
      "   2.20857237e-06 -2.03822793e-07  2.71198584e-08 -5.57309363e-07]\n",
      " [ 0.00000000e+00  0.00000000e+00 -9.80115601e-07  4.12830923e-06\n",
      "  -2.51952663e-08 -4.47482898e-06 -1.05761069e-06  8.04838464e-07\n",
      "  -4.52042786e-06  9.47923056e-08  0.00000000e+00  7.16308978e-06\n",
      "   3.76007591e-07  7.07822664e-06  3.68789665e-06 -3.14159649e-07\n",
      "   1.72112766e-06  0.00000000e+00  0.00000000e+00 -1.14813893e-05\n",
      "  -8.25416919e-06 -2.65761912e-07 -8.33451031e-06  0.00000000e+00\n",
      "   5.25000860e-06 -6.63208084e-07 -1.82243426e-07 -1.72086607e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  2.76992381e-07\n",
      "   0.00000000e+00  8.56413976e-07  2.83433769e-06  9.87760700e-07\n",
      "   4.14609417e-07  2.26082150e-07  8.35755543e-07  1.61554212e-07\n",
      "   1.09988633e-07 -1.09201358e-06  7.30338115e-07 -2.40153014e-07\n",
      "   2.76671062e-06 -3.21995600e-07 -1.04171681e-06 -1.82450219e-06\n",
      "  -1.55754844e-06  0.00000000e+00  2.69742145e-06  0.00000000e+00\n",
      "   6.41902432e-07 -1.31874242e-06 -7.51397977e-07 -4.54723158e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00 -8.78703020e-07  1.08229517e-06\n",
      "  -1.22767375e-07 -2.85739770e-06 -1.52610596e-06  0.00000000e+00\n",
      "  -1.83375471e-06 -3.29438060e-10  0.00000000e+00  3.78312689e-06\n",
      "   2.25446897e-07  2.83002714e-06  6.66554797e-07 -3.14923028e-07\n",
      "   3.11405692e-08  0.00000000e+00 -1.42025570e-07 -4.39094356e-06\n",
      "  -2.05424635e-06  4.16202788e-08 -3.33929484e-06  0.00000000e+00\n",
      "   1.87999569e-06 -1.39374787e-08  7.21308311e-08  3.42738502e-07]\n",
      " [ 0.00000000e+00  0.00000000e+00  9.26269195e-08 -3.84140520e-07\n",
      "   0.00000000e+00  2.71354850e-07  1.23232269e-06  1.57917086e-07\n",
      "   3.61680151e-07  2.81268839e-08  2.78718943e-07  0.00000000e+00\n",
      "  -1.04449313e-07 -8.21236029e-07 -2.93048686e-07 -1.86180628e-07\n",
      "   5.59743285e-07 -2.58104279e-07 -3.36850974e-07  1.27922604e-07\n",
      "   1.40102649e-08  0.00000000e+00  1.26185739e-06  0.00000000e+00\n",
      "  -1.25426126e-07 -3.48197148e-07 -1.34498492e-07 -1.45083345e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00 -2.70436288e-07  8.22181221e-07\n",
      "  -1.26240076e-08 -1.19034848e-06 -4.18060523e-07  8.07093841e-08\n",
      "  -1.01012540e-06  7.41708639e-10  0.00000000e+00  1.71658587e-06\n",
      "   5.50372690e-08  1.62194226e-06  6.69054543e-07 -8.67209238e-08\n",
      "   1.58367598e-07  0.00000000e+00  0.00000000e+00 -2.50126232e-06\n",
      "  -1.68670942e-06 -7.81319258e-08 -1.98764837e-06  0.00000000e+00\n",
      "   1.14806630e-06 -4.93777321e-08  5.07441307e-08 -1.16135752e-07]\n",
      " [ 0.00000000e+00  0.00000000e+00  2.11959809e-06 -2.45847946e-06\n",
      "  -3.56744820e-08  6.31173833e-08  1.41994053e-06  1.33331611e-07\n",
      "   2.76977289e-07 -7.07132699e-08  3.47790859e-07  0.00000000e+00\n",
      "  -1.46449759e-06 -3.06535298e-06 -2.55056648e-06  5.21356469e-07\n",
      "   5.55101735e-07 -9.58522719e-07 -1.92878611e-06  2.47255775e-06\n",
      "   0.00000000e+00  2.13163375e-07  3.46616275e-06  0.00000000e+00\n",
      "  -1.60166064e-06 -2.40915995e-07  1.33121659e-06 -3.21496532e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.26994870e-07\n",
      "   0.00000000e+00  9.68260338e-07  3.59618571e-06  8.86991359e-07\n",
      "   1.17838283e-06  1.92086759e-07  9.65298000e-07  0.00000000e+00\n",
      "  -2.94136360e-08 -2.34590721e-06 -3.10616599e-07 -4.79910454e-07\n",
      "   2.64791151e-06 -5.95025426e-07 -1.12788769e-06 -4.71893060e-07\n",
      "  -2.33644997e-07  0.00000000e+00  4.08381538e-06  0.00000000e+00\n",
      "  -1.54909525e-08 -1.37164750e-06 -7.35495973e-07 -4.99641601e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00 -1.49168741e-07  1.39005761e-06\n",
      "   0.00000000e+00 -6.68216160e-07  9.12449593e-07  5.94446021e-07\n",
      "  -1.13322631e-06  1.16934004e-07 -1.54351185e-10  1.53646579e-06\n",
      "   3.35959896e-07  1.63193658e-06  1.47332980e-06 -5.63472184e-08\n",
      "   1.33622132e-06  2.29301989e-07 -2.84150641e-08 -3.10809378e-06\n",
      "  -2.45567276e-06  0.00000000e+00 -1.51608901e-06  0.00000000e+00\n",
      "   1.38455835e-06 -5.72151066e-07 -5.47371599e-07 -1.48324023e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00 -2.58501586e-07  1.13069632e-06\n",
      "  -3.92410415e-09 -1.14995134e-06 -2.29469677e-07  2.38113088e-07\n",
      "  -1.20971333e-06  3.07367145e-08  0.00000000e+00  1.87674721e-06\n",
      "   1.17938882e-07  1.88380659e-06  1.02697231e-06 -8.56379785e-08\n",
      "   5.10330608e-07  1.78004966e-09  0.00000000e+00 -3.05829622e-06\n",
      "  -2.21789401e-06 -6.43190390e-08 -2.18493460e-06  0.00000000e+00\n",
      "   1.39639894e-06 -1.99493456e-07 -8.21099846e-08 -5.12115321e-07]\n",
      " [ 0.00000000e+00  0.00000000e+00 -5.90637455e-07  1.17182151e-06\n",
      "  -5.12440934e-08 -2.09157633e-06 -7.71086491e-07  3.20296607e-08\n",
      "  -1.56096621e-06 -1.09169651e-08  0.00000000e+00  2.84743942e-06\n",
      "   1.44588995e-07  2.52495101e-06  8.87853332e-07 -2.33952889e-07\n",
      "   5.75131551e-08  0.00000000e+00  7.23931004e-09 -3.79056769e-06\n",
      "  -2.30281489e-06 -6.12194953e-08 -3.06606171e-06  0.00000000e+00\n",
      "   1.72335933e-06 -2.84914634e-08  1.05245990e-08  8.36672527e-08]\n",
      " [ 0.00000000e+00  0.00000000e+00 -1.85166755e-07  4.94216124e-07\n",
      "  -1.19374777e-08 -7.67112169e-07 -2.70398743e-07  3.73993281e-08\n",
      "  -6.21715628e-07 -1.07255904e-09  0.00000000e+00  1.08007305e-06\n",
      "   4.25563407e-08  1.00302316e-06  3.93521219e-07 -6.42242810e-08\n",
      "   6.91531667e-08  0.00000000e+00  0.00000000e+00 -1.51891652e-06\n",
      "  -9.95575647e-07 -4.00853608e-08 -1.22644519e-06  0.00000000e+00\n",
      "   6.97542134e-07 -2.23935022e-08  2.46061944e-08 -2.90719182e-08]\n",
      " [ 0.00000000e+00  0.00000000e+00 -1.12751948e-07  6.53935274e-07\n",
      "   0.00000000e+00 -4.33020830e-07  2.00043246e-07  2.14649233e-07\n",
      "  -5.97767951e-07  3.84361343e-08  0.00000000e+00  8.43106818e-07\n",
      "   1.19578203e-07  8.92658250e-07  6.54607561e-07 -4.67952148e-08\n",
      "   4.73576335e-07  5.76035752e-08 -1.57285363e-09 -1.55051862e-06\n",
      "  -1.17486320e-06  2.54699623e-10 -9.25440702e-07  0.00000000e+00\n",
      "   7.00256805e-07 -1.97730287e-07 -1.76899590e-07 -5.04762056e-07]\n",
      " [ 0.00000000e+00  0.00000000e+00 -1.45531970e-07  4.29165141e-07\n",
      "  -7.53872254e-09 -6.33990055e-07 -2.21217277e-07  3.93672082e-08\n",
      "  -5.31124385e-07 -7.42790274e-11  0.00000000e+00  9.09423647e-07\n",
      "   2.99776346e-08  8.54077143e-07  3.46939629e-07 -4.75814694e-08\n",
      "   7.60887602e-08  0.00000000e+00  0.00000000e+00 -1.31062450e-06\n",
      "  -8.79177492e-07 -3.94016482e-08 -1.04654055e-06  0.00000000e+00\n",
      "   6.02783075e-07 -2.40970017e-08  2.63481308e-08 -5.28261452e-08]\n",
      " [ 0.00000000e+00  0.00000000e+00  4.25519842e-09 -1.24838024e-07\n",
      "   0.00000000e+00  1.22130004e-07  4.85965643e-07  9.03917012e-08\n",
      "   1.60418281e-07  1.85778735e-08  1.17970622e-07  0.00000000e+00\n",
      "  -1.28701760e-08 -3.20760876e-07 -7.86316861e-08 -7.67281563e-08\n",
      "   2.83859180e-07 -8.85464502e-08 -1.29224830e-07  1.82004223e-09\n",
      "   2.11502265e-08  0.00000000e+00  5.25473183e-07  0.00000000e+00\n",
      "  -2.62544155e-08 -1.61818818e-07 -8.61902123e-08 -6.07765514e-07]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -3.23897780e-07\n",
      "   0.00000000e+00  5.62326363e-07  2.04788921e-06  5.55246231e-07\n",
      "   6.33333684e-07  1.22128725e-07  5.67229961e-07  0.00000000e+00\n",
      "   5.13033616e-09 -1.27227088e-06 -6.84016399e-08 -2.54595903e-07\n",
      "   1.62937977e-06 -3.19741048e-07 -6.72076396e-07 -4.45581719e-07\n",
      "  -3.06539448e-07  0.00000000e+00  2.30587989e-06  0.00000000e+00\n",
      "   6.57754100e-08 -8.21832202e-07 -4.47956893e-07 -2.95117161e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.91541048e-07\n",
      "   0.00000000e+00  5.06744698e-07  1.68794691e-06  5.94530945e-07\n",
      "   2.20910991e-07  1.36397233e-07  4.96772941e-07  1.13039611e-07\n",
      "   6.62421371e-08 -6.20234971e-07  4.63458974e-07 -1.34055952e-07\n",
      "   1.66050847e-06 -1.84885053e-07 -6.25360428e-07 -1.12890291e-06\n",
      "  -9.73972078e-07  0.00000000e+00  1.57634520e-06  0.00000000e+00\n",
      "   4.01836957e-07 -7.90725892e-07 -4.48568983e-07 -2.72563875e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00 -3.92797233e-07  5.19153218e-07\n",
      "  -5.10344087e-08 -1.25277791e-06 -6.40483449e-07  0.00000000e+00\n",
      "  -8.25030043e-07 -9.48724099e-10  0.00000000e+00  1.66285076e-06\n",
      "   1.04577637e-07  1.29409705e-06  3.39271679e-07 -1.46700515e-07\n",
      "   1.21145867e-08  0.00000000e+00 -4.42877415e-08 -1.99422652e-06\n",
      "  -9.74030399e-07  1.17120020e-08 -1.53105600e-06  0.00000000e+00\n",
      "   8.56814779e-07 -6.06644868e-09  1.94667233e-08  1.47907173e-07]\n",
      " [ 0.00000000e+00  0.00000000e+00 -5.43258807e-07  2.48671631e-06\n",
      "  -1.75657267e-09 -2.39616497e-06 -3.71357345e-07  5.66090307e-07\n",
      "  -2.60458069e-06  7.86672558e-08  0.00000000e+00  3.97491931e-06\n",
      "   2.83918382e-07  4.03496915e-06  2.29189595e-06 -1.80742688e-07\n",
      "   1.22037954e-06  1.74684409e-08  0.00000000e+00 -6.58501585e-06\n",
      "  -4.81608140e-06 -1.38783562e-07 -4.60866431e-06  0.00000000e+00\n",
      "   3.00236070e-06 -4.83964357e-07 -2.41591209e-07 -1.24142900e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.04539231e-06\n",
      "   0.00000000e+00  1.12235193e-06  4.33072455e-06  9.39820950e-07\n",
      "   1.48184893e-06  1.98680311e-07  1.11804422e-06  0.00000000e+00\n",
      "  -7.49173807e-08 -2.90718435e-06 -5.90311515e-07 -6.43287592e-07\n",
      "   2.87810440e-06 -7.66103938e-07 -1.26198199e-06 -2.27266725e-07\n",
      "   7.25268166e-08  0.00000000e+00  4.87395300e-06  0.00000000e+00\n",
      "  -1.52462960e-07 -1.54818952e-06 -8.28805014e-07 -5.72050158e-06]\n",
      " [ 0.00000000e+00  0.00000000e+00 -2.52606384e-07  1.02121749e-06\n",
      "  -8.12786283e-09 -1.16713068e-06 -3.11709584e-07  1.84016514e-07\n",
      "  -1.14099771e-06  1.95115941e-08  0.00000000e+00  1.83696284e-06\n",
      "   8.25859843e-08  1.79495851e-06  8.98706332e-07 -7.93319614e-08\n",
      "   3.92315286e-07  0.00000000e+00  0.00000000e+00 -2.90194544e-06\n",
      "  -2.06998220e-06 -7.25900975e-08 -2.13637509e-06  0.00000000e+00\n",
      "   1.32868172e-06 -1.48784167e-07 -2.03893133e-08 -3.89818439e-07]\n",
      " [ 0.00000000e+00  0.00000000e+00 -2.53320962e-07  8.07383572e-07\n",
      "  -1.02274873e-08 -1.13460567e-06 -3.97486133e-07  8.90556677e-08\n",
      "  -9.80119239e-07  2.33239295e-09  0.00000000e+00  1.65512370e-06\n",
      "   4.86143144e-08  1.56989950e-06  6.64055051e-07 -7.90501673e-08\n",
      "   1.81301473e-07  0.00000000e+00  0.00000000e+00 -2.45346860e-06\n",
      "  -1.67134363e-06 -8.03704978e-08 -1.92182074e-06  0.00000000e+00\n",
      "   1.12396981e-06 -5.77505119e-08  4.96911987e-08 -1.56080702e-07]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.55452449e-06 -1.44513308e-06\n",
      "  -2.15477257e-07 -1.24732730e-06 -2.93565813e-06  9.49691170e-09\n",
      "  -9.75430112e-07  0.00000000e+00 -1.95112495e-08  7.96098789e-07\n",
      "  -1.06346045e-06 -1.04538992e-06 -1.73239255e-06  1.34398556e-06\n",
      "   3.66147290e-08  0.00000000e+00 -2.17589854e-06  1.03835146e-06\n",
      "   0.00000000e+00  4.91315973e-07  1.23082975e-06  0.00000000e+00\n",
      "  -1.10659084e-06  3.62381854e-07  1.71455781e-06 -2.45764880e-07]]\n",
      "[[ 0.0000000e+00  0.0000000e+00 -6.8582813e-05  2.8379873e-08\n",
      "   1.0448825e-05 -3.0241825e-04  1.6835787e-05  1.3499205e-04\n",
      "  -1.7991498e-04  2.0121814e-05  2.0134199e-04  4.0912200e-04\n",
      "  -1.4342141e-04  2.2660970e-08  2.7903297e-08  2.8578635e-05\n",
      "   4.5131941e-04  4.1076295e-05 -3.6770481e-04 -6.8299915e-04\n",
      "  -4.8873032e-04  5.0281305e-06  2.6098345e-04  0.0000000e+00\n",
      "   1.8182296e-04 -1.1058910e-04  1.0301854e-04 -7.4750325e-04]]\n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-8.7897956e-07  4.6155614e-07  4.1742356e-07]\n",
      " [ 9.7429336e-07  1.8722016e-06 -2.8464942e-06]\n",
      " [ 6.1975967e-08  1.6052839e-08 -7.8028805e-08]\n",
      " [-3.1392185e-06  4.4341559e-06 -1.2949372e-06]\n",
      " [-5.9212744e-07 -3.0007718e-06  3.5928992e-06]\n",
      " [ 4.2256947e-06 -5.1370621e-06  9.1136729e-07]\n",
      " [ 4.9374366e-06 -1.3521415e-06 -3.5852950e-06]\n",
      " [ 1.9845777e-06 -1.6783177e-06 -3.0625995e-07]\n",
      " [ 5.3234999e-08 -4.3175451e-08 -1.0059547e-08]\n",
      " [-1.2557771e-06  9.0033450e-07  3.5544258e-07]\n",
      " [ 1.6907185e-06 -4.2828292e-06  2.5921108e-06]\n",
      " [ 4.5463193e-07 -4.2334627e-06  3.7788309e-06]\n",
      " [ 8.9601263e-06 -1.1279791e-05  2.3196653e-06]\n",
      " [-5.8614592e-07 -1.2954769e-07  7.1569366e-07]\n",
      " [ 5.8484407e-06 -6.3663347e-06  5.1789385e-07]\n",
      " [-3.7897166e-07 -3.7092525e-07  7.4989686e-07]\n",
      " [ 6.8256918e-07 -2.4363978e-06  1.7538287e-06]\n",
      " [-4.1789331e-06  3.9212796e-06  2.5765343e-07]\n",
      " [ 2.9187885e-10  2.4892998e-07 -2.4922184e-07]\n",
      " [ 9.7946071e-08 -2.1285544e-08 -7.6660534e-08]\n",
      " [ 1.2241418e-06 -2.5610752e-06  1.3369336e-06]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.7929304e-06  1.4623254e-06  3.3060513e-07]\n",
      " [ 1.4819304e-06 -5.7737316e-06  4.2918014e-06]\n",
      " [ 2.2957915e-06 -7.9782376e-06  5.6824460e-06]\n",
      " [ 2.5331942e-06 -7.1546960e-06  4.6215014e-06]]\n",
      "[[ 1.3329554e-06  7.1129762e-07 -1.9439030e-06]]\n",
      "(2, 28)\n",
      "(1, 28)\n",
      "(28, 28)\n",
      "(1, 28)\n",
      "(28, 3)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 28)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(28, 28)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "dense3 = Layer_Dense(28, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "dense3.forward(activation2.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense3.output, y)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(loss_activation.output[:5])\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)\n",
    "\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense3.backward(loss_activation.dinputs)\n",
    "activation2.backward(dense3.dinputs)\n",
    "dense2.backward(activation2.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "# Print gradients\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbiases)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbiases)\n",
    "print(dense3.dweights)\n",
    "print(dense3.dbiases)\n",
    "\n",
    "print(dense1.dweights.shape)\n",
    "print(dense1.dbiases.shape)\n",
    "print(dense2.dweights.shape)\n",
    "print(dense2.dbiases.shape)\n",
    "print(dense3.dweights.shape)\n",
    "print(dense3.dbiases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
